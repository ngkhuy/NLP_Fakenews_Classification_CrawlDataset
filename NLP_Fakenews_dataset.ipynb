{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UYPmjQqwKHSQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hàm lấy dữ liệu\n",
        "def crawl_data_vnexpress(url):\n",
        "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # tìm tất cả trang báo có trong link\n",
        "        articles = soup.find_all('article', class_= 'item-news')\n",
        "\n",
        "        # tạo data\n",
        "        data = []\n",
        "\n",
        "        for index, article in enumerate(articles, start=1):\n",
        "\n",
        "            # lấy title của các trang báo có trong categories đó\n",
        "            title_tag = article.find('h3', class_= 'title-news') # inspect từ vnexpress\n",
        "            # nếu tìm được title của bài báo\n",
        "            if title_tag:\n",
        "                title = title_tag.get_text(strip=True) # loại dấu thừa\n",
        "\n",
        "                # lấy link của tất cả bài\n",
        "                link_tag = title_tag.find('a')\n",
        "                if link_tag and 'href' in link_tag.attrs:\n",
        "                    link = link_tag['href']\n",
        "\n",
        "                    time.sleep(2) # tránh request liên tục bị chặn\n",
        "                    # truy cập vào link bài báo\n",
        "                    response_article = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "                    if response_article. status_code == 200:\n",
        "                        article_soup = BeautifulSoup(response_article.text, 'html.parser')\n",
        "\n",
        "                        # lấy tiêu đề phụ\n",
        "                        sub_title = article_soup.find('p', class_= 'description').get_text(strip=True)\n",
        "                        # lấy nội dung bài báo\n",
        "                        content = article_soup.find_all('p', class_= 'Normal')\n",
        "                        text = '\\n'.join([t.get_text(strip=True) for t in content[:-1]]) if content else \"None\"\n",
        "\n",
        "                        # lấy tên tác giả\n",
        "                        author = content[-1].get_text(strip=True) if content else \"None\"\n",
        "\n",
        "                        # lưu data vào mảng\n",
        "                        text = text.strip()\n",
        "                        data.append([title, link, sub_title, text, author, 'real'])\n",
        "    return data\n",
        "\n",
        "def crawl_data_kenh14(url):\n",
        "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        articles = soup.find_all('h3', class_= ['ktncli-title', 'knswli-title'])\n",
        "        data = []\n",
        "\n",
        "        for index, artical in enumerate(articles, start=1):\n",
        "            link_tag = artical.find('a')\n",
        "\n",
        "            if link_tag and 'href' in link_tag.attrs:\n",
        "\n",
        "                # lấy link\n",
        "                link = \"https://kenh14.vn\" + link_tag['href']\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "                # truy cập bài báo\n",
        "                response_article = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "\n",
        "                if response_article.status_code == 200:\n",
        "                    article_soup = BeautifulSoup(response_article.text, 'html.parser')\n",
        "\n",
        "                    # lấy title\n",
        "                    title_tag = article_soup.find('h1', class_='kbwc-title')\n",
        "                    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
        "\n",
        "                    # lấy sub_title\n",
        "                    sub_title_tag = article_soup.find('h2', class_='knc-sapo')\n",
        "                    sub_title = sub_title_tag.get_text(strip=True) if sub_title_tag else \"No sub_title\"\n",
        "\n",
        "                    # lấy nội dung\n",
        "                    content_tag = article_soup.find('div', class_='detail-content')\n",
        "                    # Check if content_tag is found before proceeding\n",
        "                    if content_tag:\n",
        "                        content = content_tag.find_all('p')\n",
        "                        text = \"\\n\".join([t.get_text(strip=True) for t in content]) if content else \"No Content\"\n",
        "                    else:\n",
        "                        text = \"No Content\" # Handle case where content_tag is not found\n",
        "\n",
        "                    # lấy tác giả\n",
        "                    # Check if the 'kbwc-meta' div exists before searching for the author span\n",
        "                    author_meta_tag = article_soup.find('div', class_='kbwc-meta')\n",
        "                    if author_meta_tag:\n",
        "                        author_tag = author_meta_tag.find('span', class_='kbwcm-author')\n",
        "                        author = author_tag.get_text().strip(' ,') if author_tag else \"Unknown Author\"\n",
        "                    else:\n",
        "                        author = \"Unknown Author\"  # Handle case where author information is not found\n",
        "\n",
        "                    # lưu data\n",
        "                    text = text.strip()\n",
        "                    data.append([title, link, sub_title, text, author, 'fake'])\n",
        "    return data"
      ],
      "metadata": {
        "id": "9AfuJSasKUm6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ket noi voi drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnGdVHavIMxP",
        "outputId": "8c795bdc-278d-47b9-e185-ab6756b620e7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ham luu du lieu cao tu bai bao ve drive file csv\n",
        "import os\n",
        "\n",
        "def store_data(data, file_path):\n",
        "    df_new = pd.DataFrame(data, columns=[\"Title\", \"URL\", \"sub_title\", \"Content\", \"Author\", \"Tag\"])\n",
        "\n",
        "    # kiem tra file da ton tai chua\n",
        "    if os.path.exists(file_path):\n",
        "      # new file ton tai roi thi doc dataframe cua file\n",
        "      df_existing = pd.read_csv(file_path)\n",
        "      # gop df voi df_existing\n",
        "      df_combine = pd.concat([df_existing, df_new])\n",
        "      df_combine = df_combine.drop_duplicates(subset=\"URL\", keep=\"first\")\n",
        "\n",
        "      # shuffle data\n",
        "      df_combine = df_combine.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "      # luu vao lai file csv\n",
        "      df_combine.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n",
        "      print(f\"Save {len(df_new)} line of data to csv successfully\")\n",
        "      print(f\"Total lines of data: {len(df_combine)}\")\n",
        "    else:\n",
        "      # neu file ko ton tai thi tao moi r luu lai\n",
        "      df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
        "      df_new.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n",
        "      print(f\"Save {len(df_new)} line of data to csv successfully\")"
      ],
      "metadata": {
        "id": "GEa4joQrJH8K"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ham lay categories cua vnexpress\n",
        "def vnexpress_categories(path):\n",
        "  response = requests.get(path, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "  if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      print(\"Connect successfully\")\n",
        "\n",
        "      categories = soup.find('ul', class_='parent')\n",
        "      categories = categories.find_all('a')\n",
        "\n",
        "      # bo 2 phan tu dau va phan tu cuoi, bo luon muc dan den video vi khong can thiet\n",
        "      categories = [c['href'] for c in categories[2:-4] if c['href'] != 'https://video.vnexpress.net']\n",
        "      print(categories)\n",
        "      return categories"
      ],
      "metadata": {
        "id": "66h9JDkBTTn4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path toi drive\n",
        "file_path = \"/content/drive/MyDrive/NLP/mid_term/data.csv\""
      ],
      "metadata": {
        "id": "my3u9fBSYV7C"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lay categories cua vnexpress\n",
        "url_vnexpress = \"https://vnexpress.net\" # base url\n",
        "categories_vnexpress = vnexpress_categories(url_vnexpress)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVoYP4BhXe-x",
        "outputId": "d1f3fd16-19b5-4a22-b487-05721c47ab98"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connect successfully\n",
            "['/thoi-su', '/the-gioi', '/kinh-doanh', '/cong-nghe', '/khoa-hoc', '/podcast', '/goc-nhin', '/bat-dong-san', '/suc-khoe', '/the-thao', '/giai-tri', '/phap-luat', '/giao-duc', '/doi-song', '/oto-xe-may']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cao data vnexpress\n",
        "for category in categories_vnexpress:\n",
        "  print(f\"Dang lay du lieu tu {category} cua vnexpress\")\n",
        "  full_url = url_vnexpress + category\n",
        "  data_vnexpress = crawl_data_vnexpress(full_url)\n",
        "\n",
        "  # lay duoc 1 category thi luu vao file csv\n",
        "  store_data(data_vnexpress, file_path)\n",
        "  print(\"--------------------- \\n\")"
      ],
      "metadata": {
        "id": "aanT56NCKdZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ed575b-a2b6-402f-98cc-98241310032e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dang lay du lieu tu /thoi-su cua vnexpress\n",
            "Save 44 line of data to csv successfully\n",
            "Total lines of data: 616\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /the-gioi cua vnexpress\n",
            "Save 44 line of data to csv successfully\n",
            "Total lines of data: 618\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /kinh-doanh cua vnexpress\n",
            "Save 47 line of data to csv successfully\n",
            "Total lines of data: 620\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /cong-nghe cua vnexpress\n",
            "Save 43 line of data to csv successfully\n",
            "Total lines of data: 621\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /khoa-hoc cua vnexpress\n",
            "Save 35 line of data to csv successfully\n",
            "Total lines of data: 624\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /podcast cua vnexpress\n",
            "Save 5 line of data to csv successfully\n",
            "Total lines of data: 625\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /goc-nhin cua vnexpress\n",
            "Save 19 line of data to csv successfully\n",
            "Total lines of data: 625\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /bat-dong-san cua vnexpress\n",
            "Save 62 line of data to csv successfully\n",
            "Total lines of data: 625\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /suc-khoe cua vnexpress\n",
            "Save 47 line of data to csv successfully\n",
            "Total lines of data: 629\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /the-thao cua vnexpress\n",
            "Save 46 line of data to csv successfully\n",
            "Total lines of data: 633\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /giai-tri cua vnexpress\n",
            "Save 47 line of data to csv successfully\n",
            "Total lines of data: 633\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /phap-luat cua vnexpress\n",
            "Save 44 line of data to csv successfully\n",
            "Total lines of data: 635\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /giao-duc cua vnexpress\n",
            "Save 49 line of data to csv successfully\n",
            "Total lines of data: 637\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /doi-song cua vnexpress\n",
            "Save 41 line of data to csv successfully\n",
            "Total lines of data: 639\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu /oto-xe-may cua vnexpress\n",
            "Save 47 line of data to csv successfully\n",
            "Total lines of data: 639\n",
            "--------------------- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cào VNExpress tới du lịch thì lỗi nhưng được hơn 600 dòng nên dừng lại cào tới trang kenh14\n",
        "=> cào khoảng 15 categories thoi ahihi"
      ],
      "metadata": {
        "id": "8mbri7E8ke4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lấy categories của kenh14 cào cho nhiều bài\n",
        "\n",
        "url_kenh14 = \"https://kenh14.vn\"\n",
        "\n",
        "# hàm lấy categories của kenh14\n",
        "def kenh14_categories(path):\n",
        "  response = requests.get(path, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "  if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      print(\"Connect successfully\")\n",
        "\n",
        "      categories = soup.find('ul', class_='kbh-menu-list')\n",
        "      categories = categories.find_all('a')\n",
        "\n",
        "      categories = [c['href'] for c in categories[1:18] if c['href'] not in ['http://video.kenh14.vn/', 'javascript:;']]\n",
        "      return categories\n",
        "\n",
        "categories_kenh14 = kenh14_categories(url_kenh14)\n",
        "print(categories_kenh14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83XmfpehkmVc",
        "outputId": "fffade8f-877b-4d09-8d73-37122c168f81"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connect successfully\n",
            "['/star.chn', '/cine.chn', '/musik.chn', '/beauty-fashion.chn', '/doi-song.chn', '/money-z.chn', '/an-quay-di.chn', '/xa-hoi.chn', '/suc-khoe.chn', '/tek-life.chn', '/hoc-duong.chn', '/xem-mua-luon.chn', '/doi-song.chn', '/doi-song/nhan-vat.chn', '/xem-an-choi.chn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cao data kenh14\n",
        "for category in categories_kenh14:\n",
        "  print(f\"Dang lay du lieu tu {category} cua kenh14\")\n",
        "  full_url = url_kenh14 + category\n",
        "  data_kenh14 = crawl_data_kenh14(full_url)\n",
        "\n",
        "  # lay duoc 1 category thi luu vao file csv\n",
        "  store_data(data_kenh14, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEX65xpVod3C",
        "outputId": "55e675a1-6cb4-477d-fb3a-50c108c94b25"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dang lay du lieu tu /star.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 741\n",
            "Dang lay du lieu tu /cine.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 741\n",
            "Dang lay du lieu tu /musik.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 741\n",
            "Dang lay du lieu tu /beauty-fashion.chn cua kenh14\n",
            "Save 7 line of data to csv successfully\n",
            "Total lines of data: 741\n",
            "Dang lay du lieu tu /doi-song.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 741\n",
            "Dang lay du lieu tu /money-z.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 742\n",
            "Dang lay du lieu tu /an-quay-di.chn cua kenh14\n",
            "Save 1 line of data to csv successfully\n",
            "Total lines of data: 742\n",
            "Dang lay du lieu tu /xa-hoi.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 742\n",
            "Dang lay du lieu tu /suc-khoe.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 742\n",
            "Dang lay du lieu tu /tek-life.chn cua kenh14\n",
            "Save 7 line of data to csv successfully\n",
            "Total lines of data: 742\n",
            "Dang lay du lieu tu /hoc-duong.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 742\n",
            "Dang lay du lieu tu /xem-mua-luon.chn cua kenh14\n",
            "Save 5 line of data to csv successfully\n",
            "Total lines of data: 743\n",
            "Dang lay du lieu tu /doi-song.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 743\n",
            "Dang lay du lieu tu /doi-song/nhan-vat.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 753\n",
            "Dang lay du lieu tu /xem-an-choi.chn cua kenh14\n",
            "Save 10 line of data to csv successfully\n",
            "Total lines of data: 763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cào thêm 1 trang báo làm fake news\n",
        "# báo znews\n",
        "# hàm lấy categories của trang\n",
        "def znews_categories(path):\n",
        "  response = requests.get(path, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "  if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "      # lấy tag category\n",
        "      category_tag = soup.find('nav', class_= 'category-menu')\n",
        "      link_tag = category_tag.find_all('a')\n",
        "      link = [l['href'] for l in link_tag]\n",
        "      return link"
      ],
      "metadata": {
        "id": "abTm3QgPOMY-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "znews_links = znews_categories(\"https://znews.vn\")\n",
        "print(znews_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyXAGmvUPEEO",
        "outputId": "0b4e497a-c893-40b4-dde6-61466e2ddd8a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://znews.vn/xuat-ban.html', 'https://znews.vn/tac-gia.html', 'https://znews.vn/kinh-doanh-tai-chinh.html', 'https://znews.vn/the-thao.html', 'https://znews.vn/cong-nghe.html', 'https://lifestyle.znews.vn/suc-khoe.html', 'https://lifestyle.znews.vn/doi-song.html', 'https://znews.vn/giai-tri.html']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hàm crawl dữ liệu trang znews\n",
        "def crawl_data_znews(url):\n",
        "  response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "  if response.status_code == 200:\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # lấy tất cả bài trong trang\n",
        "    articles = soup.find_all('p', class_= 'article-title')\n",
        "\n",
        "    data = []\n",
        "    for article in articles:\n",
        "      link_tag = article.find('a')\n",
        "      if link_tag and 'href' in link_tag.attrs:\n",
        "        link = link_tag['href']\n",
        "\n",
        "        # title\n",
        "        title = link_tag.get_text(strip=True)\n",
        "        time.sleep(2)\n",
        "\n",
        "        if link:\n",
        "          response_article = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "          if response_article.status_code == 200:\n",
        "            article_soup = BeautifulSoup(response_article.text, 'html.parser')\n",
        "\n",
        "            # lấy sub_title\n",
        "            sub_title_tag = article_soup.find('p', class_='the-article-summary')\n",
        "            sub_title = sub_title_tag.get_text(strip=True) if sub_title_tag else \"No sub_title\"\n",
        "\n",
        "            # lấy content\n",
        "            content_tag = article_soup.find('div', class_='the-article-body')\n",
        "            content = content_tag.find_all('p') if content_tag else None\n",
        "            text = \"\\n\".join([t.get_text(strip=True) for t in content]) if content else \"No Content\"\n",
        "\n",
        "            # tác giả\n",
        "            author_tag = article_soup.find('li', class_='the-article-author')\n",
        "            author = author_tag.get_text(strip=True) if author_tag else \"Unknown Author\"\n",
        "\n",
        "            text = text.strip()\n",
        "            data.append([title, link, sub_title, text, author, 'fake'])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "YxV2K76zQ8Ea"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl data từ znews\n",
        "\n",
        "for link in znews_links:\n",
        "  print(f\"Dang lay du lieu tu {link} cua znews\")\n",
        "  data_znews = crawl_data_znews(link)\n",
        "\n",
        "  # lay duoc 1 category thi luu vao file csv\n",
        "  store_data(data_znews, file_path)\n",
        "  print(\"--------------------- \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2gLxxF9VKiG",
        "outputId": "fbcab473-5d79-4edd-a0cb-a55a23b4b0a9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dang lay du lieu tu https://znews.vn/xuat-ban.html cua znews\n",
            "Save 50 line of data to csv successfully\n",
            "Total lines of data: 812\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://znews.vn/tac-gia.html cua znews\n",
            "Save 50 line of data to csv successfully\n",
            "Total lines of data: 860\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://znews.vn/kinh-doanh-tai-chinh.html cua znews\n",
            "Save 67 line of data to csv successfully\n",
            "Total lines of data: 925\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://znews.vn/the-thao.html cua znews\n",
            "Save 48 line of data to csv successfully\n",
            "Total lines of data: 973\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://znews.vn/cong-nghe.html cua znews\n",
            "Save 53 line of data to csv successfully\n",
            "Total lines of data: 1026\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://lifestyle.znews.vn/suc-khoe.html cua znews\n",
            "Save 71 line of data to csv successfully\n",
            "Total lines of data: 1097\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://lifestyle.znews.vn/doi-song.html cua znews\n",
            "Save 62 line of data to csv successfully\n",
            "Total lines of data: 1158\n",
            "--------------------- \n",
            "\n",
            "Dang lay du lieu tu https://znews.vn/giai-tri.html cua znews\n",
            "Save 68 line of data to csv successfully\n",
            "Total lines of data: 1226\n",
            "--------------------- \n",
            "\n"
          ]
        }
      ]
    }
  ]
}